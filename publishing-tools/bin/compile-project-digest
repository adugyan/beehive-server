#!/usr/bin/env python3
import sys
import os
import subprocess
import argparse
import shutil
import publishing
import gzip
import csv
from datetime import datetime

# resolve repo paths
program = os.path.abspath(sys.argv[0])
program_dir = os.path.dirname(os.path.dirname(program))


def needs_refresh(pair):
    src, dst = pair

    try:
        src_mtime = os.path.getmtime(src)
    except FileNotFoundError:
        return False

    try:
        dst_mtime = os.path.getmtime(dst)
    except FileNotFoundError:
        return True

    return dst_mtime < src_mtime


def make_project_digest(data_dir, project_dir, build_dir):
    project_id = os.path.basename(project_dir)

    print('[{}] Preparing build tree.'.format(project_id))

    projdir = os.path.join(build_dir, project_id)
    os.makedirs(projdir, exist_ok=True)

    digest_dir = os.path.join(projdir, '{}.latest'.format(project_id))
    os.makedirs(digest_dir, exist_ok=True)

    staging_dir = os.path.join(projdir, 'staging')
    os.makedirs(staging_dir, exist_ok=True)

    print('[{}] Copy metadata.'.format(project_id))

    shutil.copy(os.path.join(program_dir, 'docs', 'digest-readme.md'), os.path.join(digest_dir, 'README.md'))
    shutil.copy(os.path.join(project_dir, 'nodes.csv'), os.path.join(digest_dir, 'nodes.csv'))
    shutil.copy(os.path.join(project_dir, 'sensors.csv'), os.path.join(digest_dir, 'sensors.csv'))

    print('[{}] Staging data files.'.format(project_id))

    nodes = publishing.load_project_metadata(os.path.join(program_dir, 'examples/plenario'))

    # sort candinate files in latest first order
    candidates = sorted(publishing.published_dates(nodes),
                        key=lambda item: item[1], reverse=True)

    file_pairs = []

    for node, date in candidates:
        src = os.path.join(data_dir, node['node_id'], date.strftime('%Y-%m-%d.csv.gz'))

        os.makedirs(os.path.join(projdir, 'staging', node['node_id']), exist_ok=True)
        dst = os.path.join(projdir, 'staging', node['node_id'], date.strftime('%Y-%m-%d.csv.gz'))
        file_pairs.append((src, dst))

    for src, dst in filter(needs_refresh, file_pairs):
        try:
            sensors_metadata = os.path.join(project_dir, 'sensors.csv')
            project_metadata = os.path.join(project_dir)

            script_template = '''
            gzip -dc '{src}' |
            {program_dir}/bin/filter-sensors '{sensors_metadata}' |
            {program_dir}/bin/filter-view '{project_metadata}' |
            gzip > '{dst}.tmp'
            '''

            script = script_template.format(src=src,
                                            dst=dst,
                                            program_dir=program_dir,
                                            project_dir=project_dir,
                                            sensors_metadata=sensors_metadata,
                                            project_metadata=project_metadata)

            subprocess.check_output(script, shell=True)

            os.rename(dst + '.tmp', dst)

            print('[{}] Prepared file {}.'.format(project_id, src))
        except Exception as exc:
            print('[{}] Failed to prepare file {}!'.format(project_id, src))

    print('[{}] Compiling staged files.'.format(project_id))

    with open(os.path.join(digest_dir, 'data.csv'), 'wb') as outfile:
        outfile.write(b'node_id,timestamp,plugin,sensor,parameter,value\n')

        for root, _, filenames in os.walk(staging_dir):
            for filename in filenames:
                if not filename.endswith('.csv.gz'):
                    continue
                with open(os.path.join(root, filename), 'rb') as infile:
                    data = gzip.decompress(infile.read())
                for line in data.splitlines():
                    fields = line.split(b';')
                    outfile.write(fields[0])
                    outfile.write(b',')
                    outfile.write(fields[1])
                    outfile.write(b',')
                    outfile.write(fields[2])
                    outfile.write(b',')
                    outfile.write(fields[4])
                    outfile.write(b',')
                    outfile.write(fields[5])
                    outfile.write(b',')
                    outfile.write(fields[6])
                    outfile.write(b'\n')

    print('[{}] Compiling archive.'.format(project_id))

    now = datetime.now()
    intervals = [interval for node in nodes for interval in node['commissioned']]

    data_start_date = min(interval.start for interval in intervals)
    data_end_date = max(interval.end or now for interval in intervals)

    with open(os.path.join(digest_dir, 'provenance.csv'), 'w') as outfile:
        writer = csv.writer(outfile)

        writer.writerow([
            'data_format_version',
            'project_id',
            'data_start_date',
            'data_end_date',
            'created_at',
            'url',
        ])

        writer.writerow([
            '1',
            project_id,
            data_start_date.strftime('%Y/%m/%d %H:%M:%S'),
            data_end_date.strftime('%Y/%m/%d %H:%M:%S'),
            now.strftime('%Y/%m/%d %H:%M:%S'),
            'http://mcs.anl.gov/research/projects/waggle/downloads/datasets/{}.latest.tar.gz'.format(project_id),
        ])

    shutil.make_archive(base_name=os.path.join(projdir, '{}.latest'.format(project_id)),
                        format='gztar',
                        root_dir=digest_dir)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('data_dir', help='data_dir is a directory containing the datasets tree.')
    parser.add_argument('project_dir', help='project_dir is a directory containing supporting READMEs and metadata.')
    parser.add_argument('build_dir', help='build_dir is a directory that will be used to stage data files and compile digests.')
    args = parser.parse_args()

    make_project_digest(data_dir=args.data_dir,
                        project_dir=args.project_dir,
                        build_dir=args.build_dir)
