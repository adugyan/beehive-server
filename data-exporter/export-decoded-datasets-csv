#!/usr/bin/env python3
import fileinput
from cassandra.cluster import Cluster
import os
import pipeline
import logging
import csv
import binascii
import time
from collections import Counter


def stringify(x):
    if x is None:
        return ''
    if isinstance(x, tuple) or isinstance(x, list):
        return ','.join(map(stringify, x))
    if isinstance(x, bytes) or isinstance(x, bytearray):
        return binascii.hexlify(x).decode()
    if isinstance(x, float):
        return str(round(x, 4))
    if isinstance(x, bool):
        return int(x)
    return str(x)


logger = logging.getLogger('export')
logger.setLevel(logging.INFO)

logging.basicConfig(level=logging.ERROR)

cluster = Cluster()
session = cluster.connect('waggle')
session.default_fetch_size = 50000

query = 'SELECT timestamp, plugin_name, plugin_version, parameter, data FROM sensor_data_raw WHERE node_id=%s AND date=%s'

lines = list(fileinput.input())

start_time = time.time()

old_plugins = ['coresense:3', 'alphasense:1', 'gps:1']

opened = set()

for i, line in enumerate(lines):
    try:
        fields = line.split()

        node_id = fields[0][-12:].lower()
        date = fields[2]
        partition_key = (fields[1], fields[2])

        fetch_start = time.time()
        rows = list(session.execute(query, partition_key))
        fetch_elapsed = time.time() - fetch_start

        prefix = 'datasets/2/{}'.format(node_id)
        os.makedirs(prefix, exist_ok=True)
        filename = '{}/{}.csv'.format(prefix, date)

        logger.info('{} [{:.0%}]'.format(filename, i / len(lines)))

        dataset_start_time = time.time()

        if filename in opened:
            mode = 'a'
        else:
            mode = 'w'

        opened.add(filename)

        with open(filename, mode) as outfile:
            writer = csv.writer(outfile, delimiter=';')

            row_plugins = []

            for row in rows:
                plugin = ':'.join([row.plugin_name, row.plugin_version])
                row_plugins.append(plugin)

                try:
                    results = pipeline.decode(row)
                except KeyboardInterrupt:
                    break
                except Exception as exc:
                    logger.exception('failed to decode {} {} {}'.format(node_id, date, row))
                    continue

                for sensor, sensor_values in results.items():
                    for key, value in sensor_values.items():
                        if plugin not in old_plugins:
                            if 'Chem' in sensor or 'APDS' in sensor or ('SPV1840LR5H' not in sensor and 'intensity' in key):
                                value = value.get('raw', None)
                            else:
                                value = value.get('hrf', None)

                        writer.writerow([
                            node_id,
                            row.timestamp.strftime('%Y/%m/%d %H:%M:%S'),
                            plugin,
                            row.parameter,
                            sensor,
                            key,
                            stringify(value)
                        ])

        elapsed_time = time.time() - start_time
        dataset_elapsed_time = time.time() - dataset_start_time
        count = i + 1
        mean_time = elapsed_time / count
        expected_time_left = mean_time * (len(lines) - count)

        logger.info('Total time is {:0.2f} seconds.'.format(elapsed_time))
        logger.info('Fetch time is {:0.2f} seconds.'.format(fetch_elapsed))
        logger.info('Decode time is {:0.2f} seconds.'.format(dataset_elapsed_time))
        logger.info('Mean decode time is {:0.2f} seconds.'.format(mean_time))
        logger.info('Excepted time left is {:0.2f} seconds.'.format(expected_time_left))
        logger.info('Plugin stats: {}'.format(Counter(row_plugins)))

    except KeyboardInterrupt:
        break
    except Exception as exc:
        logger.exception('Failed to decode row: {}'.format(row))
