#!/usr/bin/env python3
import fileinput
from cassandra.cluster import Cluster
import os
import pipeline
import logging
import csv
import binascii
import time
import sys


def stringify(x):
    if x is None:
        return 'NA'
    if isinstance(x, tuple) or isinstance(x, list):
        return ','.join(map(stringify, x))
    if isinstance(x, bytes) or isinstance(x, bytearray):
        return binascii.hexlify(x).decode()
    if isinstance(x, float):
        return str(round(x, 5))
    if isinstance(x, bool):
        return int(x)
    return str(x)


old_plugins = ['coresense:3', 'alphasense:1', 'gps:1']


def convertrows(rows, writer):
    for row in rows:
        plugin = row.plugin_name + ':' + row.plugin_version

        try:
            results = pipeline.decode(row)
        except KeyboardInterrupt:
            raise
        except Exception as exc:
            logger.exception('failed to decode {} {} {}'.format(node_id, date, row))
            continue

        for (subsystem, sensor), sensor_values in results.items():
            for key, value in sensor_values.items():
                if plugin in old_plugins:
                    raw_value = ''
                    hrf_value = value
                else:
                    raw_value = value.get('raw', None)
                    hrf_value = value.get('hrf', None)

                writer.writerow([
                    row.timestamp.strftime('%Y/%m/%d %H:%M:%S'),
                    node_id,
                    subsystem,
                    sensor,
                    key,
                    stringify(raw_value),
                    stringify(hrf_value),
                ])


logger = logging.getLogger('export')
logger.setLevel(logging.INFO)

cluster = Cluster(connect_timeout=30, control_connection_timeout=30)
session = cluster.connect('waggle')

query = 'SELECT timestamp, plugin_name, plugin_version, parameter, data FROM sensor_data_raw WHERE node_id=%s AND date=%s'

lines = list(fileinput.input())

opened = set()

for i, line in enumerate(lines):
    try:
        fields = line.split()

        if len(fields) != 3:
            logger.warning('skipping invalid line: {}'.format(line))
            continue

        node_id = fields[0][-12:].lower()
        date = fields[2]
        partition_key = (fields[1], fields[2])

        prefix = 'datasets/3/{}'.format(node_id)
        os.makedirs(prefix, exist_ok=True)
        filename = '{}/{}.csv'.format(prefix, date)

        if filename in opened:
            mode = 'a'
        else:
            mode = 'w'

        opened.add(filename)

        print(filename, mode)

        with open(filename, mode) as outfile:
            writer = csv.writer(outfile)

            if outfile.tell() == 0:
                writer.writerow([
                    'timestamp',
                    'node_id',
                    'subsystem',
                    'sensor',
                    'parameter',
                    'value_raw',
                    'value_hrf',
                ])

            rows = session.execute(query, partition_key)
            convertrows(rows, writer)
    except KeyboardInterrupt:
        raise
    except Exception as exc:
        logger.exception('Failed to decode row: {}'.format(row))
