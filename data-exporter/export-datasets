#!/usr/bin/env python3
import argparse
from cassandra.cluster import Cluster
import os
import logging
import csv
import binascii
import time
import sys
import datetime
from waggle.protocol.v3 import unpack_sensors as unpack_sensors_v3
from waggle.protocol.v5 import unpack_sensors as unpack_sensors_v5
from io import StringIO
import multiprocessing
import gzip


decoders = {
    ('coresense', '3'): unpack_sensors_v3,
    ('coresense', '4'): unpack_sensors_v5,
    ('status', '0'): unpack_sensors_v5,
    ('image_example', '0'): unpack_sensors_v5,
    ('spl', '0'): unpack_sensors_v5,
}


def decode_row(row):
    plugin = (row.plugin_name, row.plugin_version)

    if plugin not in decoders:
        return []

    data = binascii.unhexlify(row.data)
    return decoders[plugin](data)


def stringify(x):
    if x is None:
        return 'NA'
    if isinstance(x, tuple) or isinstance(x, list):
        return ','.join(map(stringify, x))
    if isinstance(x, bytes) or isinstance(x, bytearray):
        return binascii.hexlify(x).decode()
    if isinstance(x, float):
        return str(round(x, 5))
    if isinstance(x, bool):
        return int(x)
    return str(x)


def decode_rows(node_id, date, results, writer):
    for row in results:
        plugin = (row.plugin_name, row.plugin_version)

        try:
            samples = decode_row(row)
        except KeyboardInterrupt:
            raise
        except Exception:
            logger.exception('failed to decode {} {} {}'.format(node_id, date, row))
            continue

        for sample in samples:
            if sample.timestamp == 0:
                timestamp = row.timestamp.strftime('%Y/%m/%d %H:%M:%S')
            else:
                timestamp = sample.timestamp

            writer.writerow([
                timestamp,
                node_id,
                sample.subsystem,
                sample.sensor,
                sample.parameter,
                stringify(sample.value_raw),
                stringify(sample.value_hrf),
            ])


def make_jobs(lines):
    jobs = {}

    for line in lines:
        key = tuple(line.split())

        if len(key) != 2:
            continue

        node_id = key[0][-12:].lower()
        date = key[1]
        index = (node_id, date)

        if index not in jobs:
            jobs[index] = []

        jobs[index].append(key)

    return list(jobs.items())


def init_worker(cluster):
    global logger
    global session
    global worker_start_time
    global worker_completed

    logging.basicConfig(level=logging.CRITICAL)
    logger = multiprocessing.log_to_stderr()

    session = cluster.connect('waggle')
    print('init', session)

    worker_start_time = time.time()
    worker_completed = 0


def process_job(job):
    global logger
    global session
    global worker_start_time
    global worker_completed

    (node_id, date), partition_keys = job

    target = os.path.join(datasets_dir, node_id, date + '.csv.gz')
    print('make', target)

    start = time.time()

    write_buffer = StringIO()
    writer = csv.writer(write_buffer)
    writer.writerow([
        'timestamp',
        'node_id',
        'subsystem',
        'sensor',
        'parameter',
        'value_raw',
        'value_hrf',
    ])

    query = 'SELECT timestamp, plugin_name, plugin_version, parameter, data FROM sensor_data_raw WHERE node_id=%s AND date=%s'

    for partition_key in partition_keys:
        results = session.execute(query, partition_key)
        decode_rows(node_id, date, results, writer)

    os.makedirs(os.path.dirname(target), exist_ok=True)
    data = gzip.compress(write_buffer.getvalue().encode())

    with open(target, 'wb') as file:
        file.write(data)

    worker_completed += 1
    worker_rate = round(worker_completed / (time.time() - worker_start_time), 2)

    print('done', target, time.time() - start, worker_rate, 'datasets/s')


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('-p', '--processes', type=int, default=1)
    parser.add_argument('datasets_dir', help='Directory where datasets will be exported.')
    args = parser.parse_args()

    datasets_dir = os.path.abspath(args.datasets_dir)

    jobs = make_jobs(sys.stdin.readlines())

    cluster = Cluster()

    with multiprocessing.Pool(processes=args.processes, initializer=init_worker, initargs=(cluster,)) as pool:
        pool.map(process_job, jobs)
